# GPT-2
## Recreating the GPT-2 Model (originally in tensorflow) in PyTorch
### Building a decoder only model which is different from the Original transformer implementation
### 1024 token context length
